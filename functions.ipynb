{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain\n",
    "!pip install python-dotenv\n",
    "!pip install openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pizza_info(pizza_name: str):\n",
    "    piza_info = {\n",
    "        \"name\": pizza_name,\n",
    "        \"price\": \"10.99\"\n",
    "    }\n",
    "    return json.dumps(piza_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        #name is the name of the function that I defined\n",
    "        \"name\": \"get_pizza_info\",\n",
    "        \"description\": \"Get name and price of a pizza of the restaurant\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                # the parameter name in the function\n",
    "                \"pizza_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the pizza, e.g. Salami\",\n",
    "                },\n",
    "            },\n",
    "            # pizza_name is required since we dont have any default value for that in the function\n",
    "            \"required\": [\"pizza_name\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "my_key = openai.api_key\n",
    "def chat(query):\n",
    "    # Set your API key here (ideally it should be set as an environment variable for security reasons)\n",
    "    openai.api_key = my_key\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        functions = functions\n",
    "    )\n",
    "\n",
    "    # Assuming the response structure is correct and you're looking for a message in the response\n",
    "    message = response['choices'][0]['message']\n",
    "    return message\n",
    "\n",
    "# Example call\n",
    "print(chat(\"What is the capital of Turkey?\"))\n",
    "chat(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does pizza salami cost?\"\n",
    "message = chat(query)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does pizza salami cost?\"\n",
    "if message.get(\"function_call\"):\n",
    "    function_name = message[\"function_call\"][\"name\"]\n",
    "\n",
    "    pizza_name = json.loads(message[\"function_call\"][\"arguments\"]).get(\"pizza_name\")\n",
    "\n",
    "    function_response = get_pizza_info(\n",
    "        pizza_name= pizza_name\n",
    "    )\n",
    "\n",
    "    second_response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            message,\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response, # function_response is used to create a human like anwer with the information from our api\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "print(second_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "message = llm.predict_messages(\n",
    "    [HumanMessage(content = \"What is teh capital of France?\")], functions=functions\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much does Pizza Salami cost in the restaurant?\"\n",
    "message_pizza = llm.predict_messages([HumanMessage(content=query)], functions=functions)\n",
    "message_pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_pizza.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pizza_name = json.loads(message_pizza.additional_kwargs[\"function_call\"][\"arguments\"]).get(\"pizza_name\")\n",
    "pizza_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_api_response = get_pizza_info(pizza_name=pizza_name)\n",
    "pizza_api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = llm.predict_messages(\n",
    "    [\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=str(message_pizza.additional_kwargs)),\n",
    "        ChatMessage(\n",
    "            role = \"function\",\n",
    "            additional_kwargs={\n",
    "                \"name\": message_pizza.additional_kwargs[\"function_call\"][\"name\"]\n",
    "            },\n",
    "            content=pizza_api_response\n",
    "        ),\n",
    "    ],\n",
    "    functions = functions,\n",
    ")\n",
    "second_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tools\n",
    "you can convert Tools to functions, both the Tools provided by langchain and also your own tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import(\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun\n",
    ")\n",
    "\n",
    "class StupidJokeTool(BaseTool):\n",
    "    name = \"StupidJokeTool\"\n",
    "    description = \"Tool to explain jokes about chickens\"\n",
    "\n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        return \"It is funny, because AI...\"\n",
    "    \n",
    "    async def _arun(\n",
    "        self, query:str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"joke tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import format_tool_to_openai_function, MoveFileTool\n",
    "\n",
    "tools = [StupidJokeTool(), MoveFileTool()]\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why does the chicken cross the road? To get to the other side\"\n",
    "output = llm.predict_messages([HumanMessage(content=query)], functions=functions)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = json.loads(output.additional_kwargs[\"function_call\"][\"arguments\"]).get(\"query\")\n",
    "\n",
    "tool_response = tools[0].run(question)\n",
    "tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = llm.predict_messages(\n",
    "    [\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=str(output.additional_kwargs)),\n",
    "        ChatMessage(\n",
    "            role = \"function\",\n",
    "            additional_kwargs= {\n",
    "                \"name\": output.additional_kwargs[\"function_call\"][\"name\"]\n",
    "            },\n",
    "            content=\"\"\"\n",
    "            {tool_response}\"\"\",\n",
    "        ),\n",
    "    ],\n",
    "    functions = functions,\n",
    ")\n",
    "second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model= \"gpt-3.5-turbo\")\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm = llm, verbose=True)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Calculator\",\n",
    "        func = llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"what is 100 divided by 25?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
